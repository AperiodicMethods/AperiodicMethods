{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurodsp.sim import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General simulation settings\n",
    "n_seconds = 10\n",
    "fs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific settings\n",
    "exp = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = sim_powerlaw(n_seconds, fs, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.891866007441164"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutation_entropy(sig, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009036168596275474"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_entropy(sig, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008889862955160394"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap_entropy(sig, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from pyeeg\n",
    "# https://github.com/forrestbao/pyeeg/blob/master/pyeeg/entropy.py\n",
    "\n",
    "import numpy\n",
    "\n",
    "def ap_entropy(X, M, R):\n",
    "    \"\"\"Computer approximate entropy (ApEN) of series X, specified by M and R.\n",
    "    \n",
    "    Suppose given time series is X = [x(1), x(2), ... , x(N)]. We first build\n",
    "    embedding matrix Em, of dimension (N-M+1)-by-M, such that the i-th row of\n",
    "    Em is x(i),x(i+1), ... , x(i+M-1). Hence, the embedding lag and dimension\n",
    "    are 1 and M-1 respectively. Such a matrix can be built by calling pyeeg\n",
    "    function as Em = embed_seq(X, 1, M). Then we build matrix Emp, whose only\n",
    "    difference with Em is that the length of each embedding sequence is M + 1\n",
    "    Denote the i-th and j-th row of Em as Em[i] and Em[j]. Their k-th elements\n",
    "    are Em[i][k] and Em[j][k] respectively. The distance between Em[i] and\n",
    "    Em[j] is defined as 1) the maximum difference of their corresponding scalar\n",
    "    components, thus, max(Em[i]-Em[j]), or 2) Euclidean distance. We say two\n",
    "    1-D vectors Em[i] and Em[j] *match* in *tolerance* R, if the distance\n",
    "    between them is no greater than R, thus, max(Em[i]-Em[j]) <= R. Mostly, the\n",
    "    value of R is defined as 20% - 30% of standard deviation of X.\n",
    "    Pick Em[i] as a template, for all j such that 0 < j < N - M + 1, we can\n",
    "    check whether Em[j] matches with Em[i]. Denote the number of Em[j],\n",
    "    which is in the range of Em[i], as k[i], which is the i-th element of the\n",
    "    vector k. The probability that a random row in Em matches Em[i] is\n",
    "    \\simga_1^{N-M+1} k[i] / (N - M + 1), thus sum(k)/ (N - M + 1),\n",
    "    denoted as Cm[i].\n",
    "    \n",
    "    We repeat the same process on Emp and obtained Cmp[i], but here 0<i<N-M\n",
    "    since the length of each sequence in Emp is M + 1.\n",
    "    \n",
    "    The probability that any two embedding sequences in Em match is then\n",
    "    sum(Cm)/ (N - M +1 ). We define Phi_m = sum(log(Cm)) / (N - M + 1) and\n",
    "    Phi_mp = sum(log(Cmp)) / (N - M ).\n",
    "    And the ApEn is defined as Phi_m - Phi_mp.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Please be aware that self-match is also counted in ApEn.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    Costa M, Goldberger AL, Peng CK, Multiscale entropy analysis of biological\n",
    "    signals, Physical Review E, 71:021906, 2005\n",
    "    \n",
    "    See also\n",
    "    --------\n",
    "    samp_entropy: sample entropy of a time series\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "\n",
    "    Em = embed_seq(X, 1, M)\n",
    "    A = numpy.tile(Em, (len(Em), 1, 1))\n",
    "    B = numpy.transpose(A, [1, 0, 2])\n",
    "    D = numpy.abs(A - B)  # D[i,j,k] = |Em[i][k] - Em[j][k]|\n",
    "    InRange = numpy.max(D, axis=2) <= R\n",
    "\n",
    "    # Probability that random M-sequences are in range\n",
    "    Cm = InRange.mean(axis=0)\n",
    "\n",
    "    # M+1-sequences in range if M-sequences are in range & last values are close\n",
    "    Dp = numpy.abs(\n",
    "        numpy.tile(X[M:], (N - M, 1)) - numpy.tile(X[M:], (N - M, 1)).T\n",
    "    )\n",
    "\n",
    "    Cmp = numpy.logical_and(Dp <= R, InRange[:-1, :-1]).mean(axis=0)\n",
    "\n",
    "    Phi_m, Phi_mp = numpy.sum(numpy.log(Cm)), numpy.sum(numpy.log(Cmp))\n",
    "\n",
    "    Ap_En = (Phi_m - Phi_mp) / (N - M)\n",
    "\n",
    "    return Ap_En\n",
    "\n",
    "\n",
    "\n",
    "def permutation_entropy(x, n, tau):\n",
    "    \"\"\"Compute Permutation Entropy of a given time series x, specified by\n",
    "    permutation order n and embedding lag tau.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : list\n",
    "        Time series\n",
    "    n : integer\n",
    "        Permutation order\n",
    "    tau : integer\n",
    "        Embedding lag\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    PE : float\n",
    "       Permutation entropy\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Suppose the given time series is X =[x(1),x(2),x(3),...,x(N)].\n",
    "    We first build embedding matrix Em, of dimension(n*N-n+1),\n",
    "    such that the ith row of Em is x(i),x(i+1),..x(i+n-1). Hence\n",
    "    the embedding lag and the embedding dimension are 1 and n\n",
    "    respectively. We build this matrix from a given time series,\n",
    "    X, by calling pyEEg function embed_seq(x,1,n).\n",
    "    \n",
    "    We then transform each row of the embedding matrix into\n",
    "    a new sequence, comprising a set of integers in range of 0,..,n-1.\n",
    "    \n",
    "    The order in which the integers are placed within a row is the\n",
    "    same as those of the original elements:0 is placed where the smallest\n",
    "    element of the row was and n-1 replaces the largest element of the row.\n",
    "    \n",
    "    To calculate the Permutation entropy, we calculate the entropy of PeSeq.\n",
    "    In doing so, we count the number of occurrences of each permutation\n",
    "    in PeSeq and write it in a sequence, RankMat. We then use this sequence to\n",
    "    calculate entropy by using Shannon's entropy formula.\n",
    "    Permutation entropy is usually calculated with n in range of 3 and 7.\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural\n",
    "    complexity measure for time series.\" Physical Review Letters 88.17\n",
    "    (2002): 174102.\n",
    "    \n",
    "    Examples\n",
    "    ----------\n",
    "    >>> import pyeeg\n",
    "    >>> x = [1,2,4,5,12,3,4,5]\n",
    "    >>> pyeeg.permutation_entropy(x,5,1)\n",
    "    2.0\n",
    "    \"\"\"\n",
    "\n",
    "    PeSeq = []\n",
    "    Em = embed_seq(x, tau, n)\n",
    "\n",
    "    for i in range(0, len(Em)):\n",
    "        r = []\n",
    "        z = []\n",
    "\n",
    "        for j in range(0, len(Em[i])):\n",
    "            z.append(Em[i][j])\n",
    "\n",
    "        for j in range(0, len(Em[i])):\n",
    "            z.sort()\n",
    "            r.append(z.index(Em[i][j]))\n",
    "            z[z.index(Em[i][j])] = -1\n",
    "\n",
    "        PeSeq.append(r)\n",
    "\n",
    "    RankMat = []\n",
    "\n",
    "    while len(PeSeq) > 0:\n",
    "        RankMat.append(PeSeq.count(PeSeq[0]))\n",
    "        x = PeSeq[0]\n",
    "        for j in range(0, PeSeq.count(PeSeq[0])):\n",
    "            PeSeq.pop(PeSeq.index(x))\n",
    "\n",
    "    RankMat = numpy.array(RankMat)\n",
    "    RankMat = numpy.true_divide(RankMat, RankMat.sum())\n",
    "    EntropyMat = numpy.multiply(numpy.log2(RankMat), RankMat)\n",
    "    PE = -1 * EntropyMat.sum()\n",
    "\n",
    "    return PE\n",
    "\n",
    "def spectral_entropy(X, Band, Fs, Power_Ratio=None):\n",
    "    \"\"\"Compute spectral entropy of a time series from either two cases below:\n",
    "    1. X, the time series (default)\n",
    "    2. Power_Ratio, a list of normalized signal power in a set of frequency\n",
    "    bins defined in Band (if Power_Ratio is provided, recommended to speed up)\n",
    "    In case 1, Power_Ratio is computed by bin_power() function.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    To speed up, it is recommended to compute Power_Ratio before calling this\n",
    "    function because it may also be used by other functions whereas computing\n",
    "    it here again will slow down.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Band : list\n",
    "        boundary frequencies (in Hz) of bins. They can be unequal bins, e.g.\n",
    "        [0.5,4,7,12,30] which are delta, theta, alpha and beta respectively.\n",
    "        You can also use range() function of Python to generate equal bins and\n",
    "        pass the generated list to this function.\n",
    "        Each element of Band is a physical frequency and shall not exceed the\n",
    "        Nyquist frequency, i.e., half of sampling frequency.\n",
    "     X : list\n",
    "        a 1-D real time series.\n",
    "    Fs : integer\n",
    "        the sampling rate in physical frequency\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    As indicated in return line\n",
    "    \n",
    "    See Also\n",
    "    --------\n",
    "    bin_power: pyeeg function that computes spectral power in frequency bins\n",
    "    \"\"\"\n",
    "\n",
    "    if Power_Ratio is None:\n",
    "        Power, Power_Ratio = bin_power(X, Band, Fs)\n",
    "\n",
    "    Spectral_Entropy = 0\n",
    "    for i in range(0, len(Power_Ratio) - 1):\n",
    "        Spectral_Entropy += Power_Ratio[i] * numpy.log(Power_Ratio[i])\n",
    "    Spectral_Entropy /= numpy.log(\n",
    "        len(Power_Ratio)\n",
    "    )  # to save time, minus one is omitted\n",
    "    return -1 * Spectral_Entropy\n",
    "\n",
    "\n",
    "def svd_entropy(X, Tau, DE, W=None):\n",
    "    \"\"\"Compute SVD Entropy from either two cases below:\n",
    "    1. a time series X, with lag tau and embedding dimension dE (default)\n",
    "    2. a list, W, of normalized singular values of a matrix (if W is provided,\n",
    "    recommend to speed up.)\n",
    "    If W is None, the function will do as follows to prepare singular spectrum:\n",
    "        First, computer an embedding matrix from X, Tau and DE using pyeeg\n",
    "        function embed_seq():\n",
    "                    M = embed_seq(X, Tau, DE)\n",
    "        Second, use scipy.linalg function svd to decompose the embedding matrix\n",
    "        M and obtain a list of singular values:\n",
    "                    W = svd(M, compute_uv=0)\n",
    "        At last, normalize W:\n",
    "                    W /= sum(W)\n",
    "    Notes\n",
    "    -------------\n",
    "    To speed up, it is recommended to compute W before calling this function\n",
    "    because W may also be used by other functions whereas computing it here\n",
    "    again will slow down.\n",
    "    \"\"\"\n",
    "\n",
    "    if W is None:\n",
    "        Y = embed_seq(X, Tau, DE)\n",
    "        W = numpy.linalg.svd(Y, compute_uv=0)\n",
    "        W /= sum(W)  # normalize singular values\n",
    "\n",
    "    return -1 * sum(W * numpy.log(W))\n",
    "\n",
    "def samp_entropy(X, M, R):\n",
    "    \"\"\"Computer sample entropy (SampEn) of series X, specified by M and R.\n",
    "    \n",
    "    SampEn is very close to ApEn.\n",
    "    \n",
    "    Suppose given time series is X = [x(1), x(2), ... , x(N)]. We first build\n",
    "    embedding matrix Em, of dimension (N-M+1)-by-M, such that the i-th row of\n",
    "    Em is x(i),x(i+1), ... , x(i+M-1). Hence, the embedding lag and dimension\n",
    "    are 1 and M-1 respectively. Such a matrix can be built by calling pyeeg\n",
    "    function as Em = embed_seq(X, 1, M). Then we build matrix Emp, whose only\n",
    "    difference with Em is that the length of each embedding sequence is M + 1\n",
    "    \n",
    "    Denote the i-th and j-th row of Em as Em[i] and Em[j]. Their k-th elements\n",
    "    are Em[i][k] and Em[j][k] respectively. The distance between Em[i] and\n",
    "    Em[j] is defined as 1) the maximum difference of their corresponding scalar\n",
    "    components, thus, max(Em[i]-Em[j]), or 2) Euclidean distance. We say two\n",
    "    1-D vectors Em[i] and Em[j] *match* in *tolerance* R, if the distance\n",
    "    between them is no greater than R, thus, max(Em[i]-Em[j]) <= R. Mostly, the\n",
    "    value of R is defined as 20% - 30% of standard deviation of X.\n",
    "    \n",
    "    Pick Em[i] as a template, for all j such that 0 < j < N - M , we can\n",
    "    check whether Em[j] matches with Em[i]. Denote the number of Em[j],\n",
    "    which is in the range of Em[i], as k[i], which is the i-th element of the\n",
    "    vector k.\n",
    "    \n",
    "    We repeat the same process on Emp and obtained Cmp[i], 0 < i < N - M.\n",
    "    The SampEn is defined as log(sum(Cm)/sum(Cmp))\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    Costa M, Goldberger AL, Peng C-K, Multiscale entropy analysis of biological\n",
    "    signals, Physical Review E, 71:021906, 2005\n",
    "    \n",
    "    See also\n",
    "    --------\n",
    "    ap_entropy: approximate entropy of a time series\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(X)\n",
    "\n",
    "    Em = embed_seq(X, 1, M)\n",
    "    A = numpy.tile(Em, (len(Em), 1, 1))\n",
    "    B = numpy.transpose(A, [1, 0, 2])\n",
    "    D = numpy.abs(A - B)  # D[i,j,k] = |Em[i][k] - Em[j][k]|\n",
    "    InRange = numpy.max(D, axis=2) <= R\n",
    "    numpy.fill_diagonal(InRange, 0)  # Don't count self-matches\n",
    "\n",
    "    Cm = InRange.sum(axis=0)  # Probability that random M-sequences are in range\n",
    "    Dp = numpy.abs(\n",
    "        numpy.tile(X[M:], (N - M, 1)) - numpy.tile(X[M:], (N - M, 1)).T\n",
    "    )\n",
    "\n",
    "    Cmp = numpy.logical_and(Dp <= R, InRange[:-1, :-1]).sum(axis=0)\n",
    "\n",
    "    # Avoid taking log(0)\n",
    "    Samp_En = numpy.log(numpy.sum(Cm + 1e-100) / numpy.sum(Cmp + 1e-100))\n",
    "\n",
    "    return Samp_En\n",
    "\n",
    "\n",
    "def embed_seq(time_series, tau, embedding_dimension):\n",
    "    \"\"\"Build a set of embedding sequences from given time series `time_series`\n",
    "    with lag `tau` and embedding dimension `embedding_dimension`.\n",
    "    Let time_series = [x(1), x(2), ... , x(N)], then for each i such that\n",
    "    1 < i <  N - (embedding_dimension - 1) * tau,\n",
    "    we build an embedding sequence,\n",
    "    Y(i) = [x(i), x(i + tau), ... , x(i + (embedding_dimension - 1) * tau)].\n",
    "    All embedding sequences are placed in a matrix Y.\n",
    "    Parameters\n",
    "    ----------\n",
    "    time_series\n",
    "        list or numpy.ndarray\n",
    "        a time series\n",
    "    tau\n",
    "        integer\n",
    "        the lag or delay when building embedding sequence\n",
    "    embedding_dimension\n",
    "        integer\n",
    "        the embedding dimension\n",
    "    Returns\n",
    "    -------\n",
    "    Y\n",
    "        2-embedding_dimension list\n",
    "        embedding matrix built\n",
    "    Examples\n",
    "    ---------------\n",
    "    >>> import pyeeg\n",
    "    >>> a=range(0,9)\n",
    "    >>> pyeeg.embed_seq(a,1,4)\n",
    "    array([[0,  1,  2,  3],\n",
    "           [1,  2,  3,  4],\n",
    "           [2,  3,  4,  5],\n",
    "           [3,  4,  5,  6],\n",
    "           [4,  5,  6,  7],\n",
    "           [5,  6,  7,  8]])\n",
    "    >>> pyeeg.embed_seq(a,2,3)\n",
    "    array([[0,  2,  4],\n",
    "           [1,  3,  5],\n",
    "           [2,  4,  6],\n",
    "           [3,  5,  7],\n",
    "           [4,  6,  8]])\n",
    "    >>> pyeeg.embed_seq(a,4,1)\n",
    "    array([[0],\n",
    "           [1],\n",
    "           [2],\n",
    "           [3],\n",
    "           [4],\n",
    "           [5],\n",
    "           [6],\n",
    "           [7],\n",
    "           [8]])\n",
    "    \"\"\"\n",
    "    if not type(time_series) == numpy.ndarray:\n",
    "        typed_time_series = numpy.asarray(time_series)\n",
    "    else:\n",
    "        typed_time_series = time_series\n",
    "\n",
    "    shape = (\n",
    "        typed_time_series.size - tau * (embedding_dimension - 1),\n",
    "        embedding_dimension\n",
    "    )\n",
    "\n",
    "    strides = (typed_time_series.itemsize, tau * typed_time_series.itemsize)\n",
    "\n",
    "    return numpy.lib.stride_tricks.as_strided(\n",
    "        typed_time_series,\n",
    "        shape=shape,\n",
    "        strides=strides\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
